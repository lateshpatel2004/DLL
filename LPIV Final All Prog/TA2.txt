# -----------------------------------------
# Step 1: Import the necessary packages
# -----------------------------------------
import numpy as np                      # For numerical operations and array handling
import tensorflow as tf                 # Deep learning framework
from tensorflow.keras.datasets import mnist   # MNIST dataset loader
from tensorflow.keras.models import Sequential # Sequential model type (layer-by-layer)
from tensorflow.keras.layers import Dense      # Fully connected (dense) layer
import matplotlib.pyplot as plt          # For plotting graphs

# -----------------------------------------
# Step 2: Load and preprocess the MNIST dataset
# -----------------------------------------
print("[INFO] accessing MNIST...")

# Load training and testing data from MNIST
# Each image is 28x28 grayscale, and labels range from 0–9 (digits)
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Flatten each 28x28 image into a single 784-dimensional vector
# Convert from integers (0–255) to floats (0–1) for normalization
x_train = x_train.reshape((x_train.shape[0], -1)).astype('float32') / 255
x_test = x_test.reshape((x_test.shape[0], -1)).astype('float32') / 255

# Convert integer class labels (0–9) into one-hot encoded vectors
# e.g., label 3 → [0,0,0,1,0,0,0,0,0,0]
y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

# -----------------------------------------
# Step 3: Define the Neural Network architecture
# -----------------------------------------
# We'll use a simple feed-forward neural network (Multilayer Perceptron)
# with 3 hidden layers (each having 64 neurons) and ReLU activations.
# The output layer has 10 neurons (for the 10 digit classes) and softmax activation.

model = Sequential([
    Dense(64, activation='relu', input_shape=(784,)),  # Input layer (784 → 64)
    Dense(64, activation='relu'),                      # Hidden layer 1
    Dense(64, activation='relu'),                      # Hidden layer 2
    Dense(10, activation='softmax')                    # Output layer (10 classes)
])

# Ignore potential TensorFlow warnings for clarity
import warnings
warnings.filterwarnings("ignore")

# -----------------------------------------
# Step 4: Compile the model
# -----------------------------------------
# Define optimizer, loss function, and metrics to monitor during training
# - Optimizer: Stochastic Gradient Descent (SGD)
# - Loss: Categorical Crossentropy (used for multi-class classification)
# - Metrics: Accuracy (for tracking performance)
model.compile(
    optimizer='sgd',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# -----------------------------------------
# Step 5: Train the model
# -----------------------------------------
# Fit the model to the training data
# - epochs: number of full passes through the dataset (15 here)
# - batch_size: number of samples per gradient update (32 here)
# - validation_data: test data used to evaluate performance during training
H = model.fit(
    x_train, y_train,
    epochs=15,
    batch_size=32,
    validation_data=(x_test, y_test)
)

# -----------------------------------------
# Step 6: Evaluate the trained network on test data
# -----------------------------------------
# Calculate final accuracy and loss on the test set
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_accuracy*100:.2f}%')

# -----------------------------------------
# Step 7: Visualize training performance
# -----------------------------------------
# Plot training loss and accuracy trends over epochs
plt.style.use("ggplot")
plt.figure(figsize=(10, 4))

# Plot both loss and accuracy curves
plt.subplot(1, 2, 1)
plt.plot(H.history['loss'], label="Training Loss")
plt.plot(H.history['accuracy'], label="Training Accuracy")
plt.title("Training Loss and Accuracy over Epochs")
plt.xlabel("Epoch #")
plt.ylabel("Loss / Accuracy")
plt.legend()

# Optionally, you can also plot validation loss/accuracy to compare:
plt.subplot(1, 2, 2)
plt.plot(H.history['val_loss'], label="Validation Loss")
plt.plot(H.history['val_accuracy'], label="Validation Accuracy")
plt.title("Validation Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss / Accuracy")
plt.legend()

plt.tight_layout()
plt.show()
