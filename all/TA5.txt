# ------------------------------------------------------------
# Continuous Bag of Words (CBOW) Example with WordCloud Visualization
# ------------------------------------------------------------

# Step 1: Import the necessary libraries
import re                      # For text cleaning and preprocessing (regex)
import numpy as np             # For numerical operations and arrays
import string                  # Useful string constants (not used directly here)
import pandas as pd            # Data handling (not used here but good practice to import)
import matplotlib as mpl        # Base for matplotlib
import matplotlib.pyplot as plt # For visualizations

# This line ensures plots appear inline in Jupyter notebooks.
# If running as a .py script, you can remove or ignore it.
%matplotlib inline

from subprocess import check_output  # Typically used for running shell commands (not used here)
from wordcloud import WordCloud, STOPWORDS  # For generating WordClouds

# ------------------------------------------------------------
# Step 2: Prepare stopwords and text data
# ------------------------------------------------------------

# Define a set of stopwords to exclude common words (e.g., 'the', 'is', 'and') from WordCloud
stopwords = set(STOPWORDS)

# Sample text to analyze (a small passage about computational processes)
sentences = """We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""

# ------------------------------------------------------------
# Step 3: Generate a WordCloud visualization
# ------------------------------------------------------------

# Create a WordCloud object with styling and generation parameters
Wordcloud = WordCloud(
    background_color='white',  # white background for clarity
    stopwords=stopwords,       # remove common words
    max_words=200,             # show up to 200 words
    max_font_size=40,          # largest font size
    random_state=42            # seed for reproducibility
).generate(sentences)          # generate the word cloud from the text

# Create and display the WordCloud using matplotlib
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
axes.imshow(Wordcloud)  # Render the word cloud image
axes.axis('off')        # Hide axes for a clean look
fig.tight_layout()      # Adjust spacing
plt.show()

# ------------------------------------------------------------
# Step 4: Text preprocessing for CBOW
# ------------------------------------------------------------

# Reuse the same text (used both for visualization and model input)
sentences = """We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""

# 1️⃣ Remove special characters and punctuation:
# Keep only letters and numbers, replacing other characters with spaces.
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)

# 2️⃣ Remove single-letter words (like “a” or “I”) — they often add noise.
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()

# 3️⃣ Convert everything to lowercase — ensures consistency.
sentences = sentences.lower()
print("Cleaned Text:\n", sentences, "\n")

# 4️⃣ Split text into individual words (tokenization)
words = sentences.split()

# 5️⃣ Create a vocabulary (unique set of words)
vocab = set(words)

# Display the tokens and vocabulary
print("Tokenized Words:\n", words, "\n")
print("Vocabulary (unique words):\n", vocab, "\n")

# ------------------------------------------------------------
# Step 5: Build vocabulary mapping and CBOW data
# ------------------------------------------------------------

# Vocabulary size (number of unique tokens)
vocab_size = len(vocab)
print("Vocabulary size:", vocab_size)

# Embedding dimensionality — each word will be represented as a vector of this length
embed_dim = 10

# Context window size — how many words on each side to consider
context_size = 2   # means: use 2 before and 2 after (4 total context words)

# Create mapping dictionaries:
# word_to_ix: map each word to an index (integer)
# ix_to_word: reverse mapping (index back to word)
word_to_ix = {word: i for i, word in enumerate(vocab)}
ix_to_word = {i: word for i, word in enumerate(vocab)}

print("Word to Index Mapping:\n", word_to_ix, "\n")
print("Index to Word Mapping:\n", ix_to_word, "\n")

# ------------------------------------------------------------
# Step 6: Generate training data for CBOW
# ------------------------------------------------------------
# CBOW = Continuous Bag of Words: 
# Predict target word based on context words (surrounding words)

data = []  # will hold tuples of ([context words], target word)

# Iterate through the word list, skipping the first two and last two words
# so we can take two words before and after as context.
for i in range(2, len(words) - 2):
    # select context words (2 before and 2 after)
    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]
    # target word is the one in the middle
    target = words[i]
    # append to dataset
    data.append((context, target))

# Display first 5 examples of (context, target) pairs
print("Sample CBOW Training Data (context, target):\n", data[:5], "\n")

# ------------------------------------------------------------
# Step 7: Initialize random word embeddings
# ------------------------------------------------------------
# Create a random embedding matrix:
# Shape = (vocab_size, embed_dim)
# Each word (by index) is represented by a vector of length `embed_dim`.
embeddings = np.random.random_sample((vocab_size, embed_dim))

# Display sample embeddings
print("Initial Random Embeddings:\n", embeddings)
